{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wagner-Jackson-ChurnCompetition-FASAM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF1Usa0-dPT9",
        "colab_type": "text"
      },
      "source": [
        "# FASAM - CHURN COMPETITION \n",
        "Alunos: Wagner Cristian de Siqueira / Jackson Dias Savitraz \n",
        " \n",
        "## Introdução \n",
        "O *churn*, também conhecido como *churn rate*, é um indicador que mede o índice de evasão dos clientes, ou seja, a taxa de cancelamento. \n",
        "Isso quer dizer que o famoso *churn* é o responsável por dizer para os gestores quantos clientes eles estão perdendo, seja por mês, a cada 6 meses ou por ano. \n",
        "Ele é importante porque, se sua empresa está perdendo muitos clientes, ou seja, está com o *churn* alto, é sinal de que há algum problema que precisa ser melhorado. \n",
        "Pode ser o seu produto, que não está adequado às expectativas dos clientes, pode ser o atendimento da sua equipe, que está deixando a desejar… enfim, é preciso investigar onde está o ponto fraco. \n",
        " \n",
        "## Objetivo \n",
        " Um banco está investigando uma taxa muito alta de churn dos seus clientes. \n",
        " Aqui está um conjunto de dados de 10.000 registros para investigar e prever quais clientes estão mais propensos a sair do banco em breve. \n",
        "  \n",
        "## Modelo\n",
        "O XGBoost (*Extreme Gradient Boosting*), baseado no projeto de pesquisa \"*XGBoost: A Scalable Tree Boosting System*\" proposto por Tianqi Chen e Carlos Guestrin, é o algoritmo mais popular de machine learning atualmente. \n",
        "Utiliza um conjunto de métodos baseados em árvores de decisão, reunidos em uma biblioteca projetada e otimizada para para extrair o máximo de performance das arquiteturas computacionais disponíveis e criar um modelo mais geral. \n",
        "O mecanismo utilizado pelo XGBoost como modelo de escolha é chamado *ensemble* de árvores de decisão, ou seja, um conjunto de árvores de classificação e regressão denominado CART (*Classification and Regression Trees*). \n",
        "Com integração para Python através do framework Scikit-learn, este algoritmo é o estado da arte para problemas de classificação e ganhou muita popularidade devido ser escolha de muitas equipes vencedoras em competições de aprendizagem de máquina no Kaggle. \n",
        " \n",
        "## Conclusão\n",
        "Através da utilização do modelo que atualmente é o estado da arte em problemas de classificação, conseguimos um F-Score médio maior que 0,9 para o dataset de validação e, consequentemente, o primeiro lugar do desafio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVwKkbeENd7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importa as bibliotecas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "#função para verificar o treinamento do modelo\n",
        "def eval_metrics(actual, pred):\n",
        "    rmse = np.sqrt(mean_squared_error(actual, pred.round()))\n",
        "    mae  = mean_absolute_error(actual, pred.round())\n",
        "    r2   = r2_score(actual, pred.round())\n",
        "    f1   = f1_score(actual, pred.round())\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"R2:\", r2)\n",
        "    print(\"F1:\", f1)\n",
        "\n",
        "#função para salvar o arquivo de submissão\n",
        "def save_submission(df, name='submission.csv'):\n",
        "    with open(name, 'w+') as f:\n",
        "        f.write(\"RowNumber,Exited\\n\")    \n",
        "        i = 0\n",
        "        for k, row in df.iterrows():\n",
        "            f.write(\"{},{}\\n\".format(row.RowNumber, row.Exited))  \n",
        "            i = i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOA0FmvXNQgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#carrega o dataset de treino\n",
        "dataset = pd.read_csv('train.csv')\n",
        "\n",
        "#para as entradas, descartamos as colunas RowNumber, CustomerId, Surname e Exited\n",
        "X = dataset.iloc[:,3:-1].values\n",
        "\n",
        "#para a saída, utilizamos a coluna Exited\n",
        "y = dataset.iloc[:,-1].values\n",
        "\n",
        "#utiliza o label encoder para as colunas Geography e Gender\n",
        "le = LabelEncoder()\n",
        "X[:,1] = le.fit_transform(X[:,1])\n",
        "X[:,2] = le.fit_transform(X[:,2])\n",
        "\n",
        "#utiliza o one-hot encoder para gerar as colunas de features\n",
        "preprocessor = make_column_transformer((OneHotEncoder(categories='auto'),[1]),remainder='passthrough')\n",
        "X = preprocessor.fit_transform(X)\n",
        "\n",
        "#utiliza o standard scaler para normalizar as colunas valoradas\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Hbud4MXEC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#quebra o dataset em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EFPOHEzX5xJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "53c86024-660b-4e21-b79b-114f5efe541c"
      },
      "source": [
        "#treina o modelo com as entradas e saída\n",
        "model = XGBClassifier(max_depth=8)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#verifica o treinamento do modelo\n",
        "y_pred = model.predict(X_test)\n",
        "eval_metrics(y_test, y_pred)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.36639108100829126\n",
            "MAE: 0.13424242424242425\n",
            "R2: 0.14431259661529683\n",
            "F1: 0.5840375586854459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNtwZVsmamA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#carrega o dataset de validação\n",
        "dataset = pd.read_csv('valid.csv')\n",
        "\n",
        "#para as entradas, descartamos as colunas RowNumber, CustomerId e Surname\n",
        "X_valid = dataset.iloc[:,3:].values\n",
        "\n",
        "#utiliza o label encoder para as colunas Geography e Gender\n",
        "X_valid[:,1] = le.fit_transform(X_valid[:,1])\n",
        "X_valid[:,2] = le.fit_transform(X_valid[:,2])\n",
        "\n",
        "#utiliza o one-hot encoder para gerar as colunas de features\n",
        "X_valid = preprocessor.fit_transform(X_valid)\n",
        "\n",
        "#utiliza o standard scaler para normalizar as colunas valoradas\n",
        "X_valid = sc.fit_transform(X_valid)\n",
        "\n",
        "#treina o modelo com todo dataset\n",
        "model.fit(X, y)\n",
        "\n",
        "#faz a predição do dataset de validação\n",
        "pred = model.predict(X_valid)\n",
        "\n",
        "#armazena o valor gerado para a coluna Exited\n",
        "dataset['Exited'] = pred\n",
        "\n",
        "#salva o arquivo de submissão para o Kaggle\n",
        "save_submission(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}